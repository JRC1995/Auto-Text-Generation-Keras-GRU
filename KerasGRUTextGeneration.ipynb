{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter file name for training and testing data(make sure it's in the same directory): Witt2.txt\n",
      "Total Characters:  55233\n",
      "Vocabulary:  50\n",
      "Do you want to see the character to integer map? (y/n): n\n",
      "\n",
      "Length of sample text: 400\n",
      "\n",
      "Choose:\n",
      "Enter 0 if you want train the model.\n",
      "Enter 1 if you want to load saved model and weights.\n",
      "Enter: 0\n",
      "\n",
      "Number of Hidden Layers: 2\n",
      "Number of Neurons in Hidden Layer 1: 50\n",
      "Number of Neurons in Hidden Layer 2: 50\n",
      "Time Steps: 100\n",
      "Learning Rate: 0.01\n",
      "Dropout Rate: 0.2\n",
      "Training Batch Size: 125\n",
      "Total Patterns:  55133\n",
      "\n",
      "Building model...\n",
      "\n",
      "Iteration:  1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Multilayered (or singlelayered) GRU based RNN for text generation using Keras libraries\n",
    "# Tested in Tensorflow backend library\n",
    "# Code by: Jishnu Ray Chowdhury\n",
    "# License: BSD\n",
    "\n",
    "# import libraries\n",
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import GRU\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# load Dataset\n",
    "\"\"\"path = \"Witt2.txt\" # Extract from Wittgenstein's 'Philosophical Investigations'\"\"\"\n",
    "\n",
    "path = raw_input(\"Enter file name for training and testing data(make sure it's in the same directory): \")\n",
    "dataset = open(path).read().lower()\n",
    "\n",
    "# store the list of all unique characters in dataset\n",
    "chars = sorted(list(set(dataset)))\n",
    "\n",
    "total_chars = len(dataset)\n",
    "vocabulary = len(chars)\n",
    "\n",
    "print(\"Total Characters: \", total_chars)\n",
    "print(\"Vocabulary: \", vocabulary)\n",
    "\n",
    "# Creating dictionary or map in order to map all characters to an integer and vice versa\n",
    "char_to_int = { c:i for i, c in enumerate(chars)}\n",
    "int_to_char = { i:c for i, c in enumerate(chars)}\n",
    "\n",
    "mapvar = raw_input(\"Do you want to see the character to integer map? (y/n): \")\n",
    "\n",
    "if mapvar == \"y\" or mapvar == \"Y\":\n",
    "    # Show the map from Char to Int\n",
    "    print('\\nGenerated Map: ')\n",
    "    for char in chars:\n",
    "        print(char,' is mapped to ',char_to_int[char],' and vice versa.')\n",
    "\n",
    "# Asking the important questions\n",
    "sample_len = int(raw_input(\"\\nLength of sample text: \"))\n",
    "print(\"\\nChoose:\")\n",
    "print(\"Enter 0 if you want train the model.\")\n",
    "print(\"Enter 1 if you want to load saved model and weights.\")\n",
    "Answer = int(raw_input('Enter: '))\n",
    "\n",
    "if Answer == 0:\n",
    "    \n",
    "    # Tested with:\n",
    "    # Number of Hidden Layers: 1\n",
    "    # Number of Neurons in Hidden Layer 1: 100\n",
    "    # Time Steps: 30\n",
    "    # Learning Rate: 0.01\n",
    "    # Dropout Rate: 0.2\n",
    "    # Batch Size: 125 or something\n",
    "    # Do try other combinations\n",
    "    \n",
    "    hidden_layers = int(raw_input(\"\\nNumber of Hidden Layers: \"))\n",
    "    neurons = []\n",
    "    for i in xrange(0,hidden_layers):\n",
    "        neurons.append(int(raw_input(\"Number of Neurons in Hidden Layer \"+str(i+1)+\": \")))\n",
    "    seq_len = int(raw_input(\"Time Steps: \"))\n",
    "    learning_rate = float(raw_input(\"Learning Rate: \"))\n",
    "    dropout_rate = float(raw_input(\"Dropout Rate: \"))\n",
    "    batch = int(raw_input(\"Training Batch Size: \"))\n",
    "\n",
    "    # prepare input data and output(target) data\n",
    "    # (X signified Inputs and Y signifies Output(targeted-output in this case))\n",
    "    dataX = []   \n",
    "    dataY = []\n",
    "\n",
    "    for i in range(0,total_chars-seq_len):  # Example of an extract of dataset: Language \n",
    "        dataX.append(dataset[i:i+seq_len])  # Example Input Data: Languag\n",
    "        dataY.append(dataset[i+seq_len])    # Example of corresponding Target Output Data: e\n",
    "\n",
    "    total_patterns = len(dataX) \n",
    "    print(\"Total Patterns: \", total_patterns)\n",
    "\n",
    "    # One Hot Encoding...\n",
    "    X = np.zeros((total_patterns, seq_len, vocabulary), dtype=np.bool)\n",
    "    Y = np.zeros((total_patterns, vocabulary), dtype=np.bool)\n",
    "\n",
    "    for pattern in xrange(total_patterns):\n",
    "        for seq_pos in xrange(seq_len):\n",
    "            vocab_index = char_to_int[dataX[pattern][seq_pos]]\n",
    "            X[pattern,seq_pos,vocab_index] = 1\n",
    "        vocab_index = char_to_int[dataY[pattern]]\n",
    "        Y[pattern,vocab_index] = 1\n",
    "    \n",
    "# build the model: a multi(or single depending on user input)-layered GRU based RNN\n",
    "print('\\nBuilding model...')\n",
    "\n",
    "if Answer == 0:\n",
    "    model = Sequential()\n",
    "\n",
    "    if hidden_layers == 1:\n",
    "        model.add(GRU(neurons[0], input_shape=(seq_len,vocabulary)))\n",
    "    else:\n",
    "        model.add(GRU(neurons[0], input_shape=(seq_len,vocabulary),return_sequences=True))\n",
    "\n",
    "    for i in xrange(1,hidden_layers):\n",
    "        model.add(GRU(neurons[i]))\n",
    "    \n",
    "    model.add(Dense(vocabulary))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    RMSprop_optimizer = RMSprop(lr=learning_rate)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=RMSprop_optimizer)\n",
    "\n",
    "    # define the checkpoint\n",
    "    filepath=\"GRUWeights.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "    \n",
    "    # save the model information\n",
    "    model.save('GRUModel.h5')\n",
    "    f = open('GRUTimeStep','w+')\n",
    "    f.write(str(seq_len))\n",
    "    f.close()\n",
    "    \n",
    "else:\n",
    "    try:\n",
    "        model = load_model('GRUModel.h5')\n",
    "    except:\n",
    "        print(\"\\nUh Oh! Caught some exceptions! May be you don't have any trained and saved model to load.\")\n",
    "        print(\"Solution: May be create and train the model anew ?\")\n",
    "        sys.exit(0)\n",
    "    try:\n",
    "        seq_len = int(open('GRUTimeStep').read())\n",
    "    except:\n",
    "        print(\"\\nUh Oh! Caught some exceptions! May be you are missing the file having time step information\")\n",
    "        seq_len = int(raw_input(\"Time Steps (I hope, you remember what it was): \"))\n",
    "        f = open('GRUTimeStep','w+')\n",
    "        f.write(str(seq_len))\n",
    "        f.close()\n",
    "    \n",
    "# Function for creating a sample text from a random seed (an extract from the dataset).\n",
    "# The seed acts as the input for the GRU RNN and after feed forwarding through the network it produces the output\n",
    "# (the output can be considered to be the prediction for the next character)\n",
    "\n",
    "def sample(seed):\n",
    "    \n",
    "    # One hot encoding the input seed\n",
    "    for i in xrange(sample_len):\n",
    "            x = np.zeros((1, seq_len, vocabulary))\n",
    "            for seq_pos in xrange(seq_len):\n",
    "                vocab_index = char_to_int[seed[seq_pos]]\n",
    "                x[0,seq_pos,vocab_index] = 1\n",
    "            \n",
    "            # procuring the output (or prediction) from the network\n",
    "            prediction = model.predict(x, verbose=0)\n",
    "            \n",
    "            # The prediction is an array of probabilities for each unique characters. \n",
    "            # Randomly an integer(mapped to a character) is chosen based on its likelihood \n",
    "            # as described in prediction list\n",
    "            \n",
    "            RNG_int = np.random.choice(range(vocabulary), p=prediction.ravel())          \n",
    "            \n",
    "            # The next character (to generate) is mapped to the randomly chosen integer \n",
    "            # Procuring the next character from the dictionary by putting in the chosen integer\n",
    "            next_char = int_to_char[RNG_int]\n",
    "          \n",
    "            # Display the chosen character\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()            \n",
    "            \n",
    "            # modifying seed for the next iteration for finding the next character\n",
    "            seed = seed[1:] + next_char\n",
    "            \n",
    "    print()\n",
    "            \n",
    "\n",
    "if Answer == 0:\n",
    "    # Train Model and print sample text at each epoch.\n",
    "    for iteration in range(1, 60):\n",
    "        print()\n",
    "        print('Iteration: ', iteration)\n",
    "        print()\n",
    "        \n",
    "        # Train model. If you have forgotten: X = input, Y = targeted outputs\n",
    "        model.fit(X, Y, batch_size=batch, nb_epoch=1, callbacks=callbacks_list)\n",
    "        print()\n",
    "        \n",
    "        # Randomly choosing a sequence from dataset to serve as a seed for sampling\n",
    "        start_index = random.randint(0, total_chars - seq_len - 1)\n",
    "        seed = dataset[start_index: start_index + seq_len]\n",
    "        \n",
    "        sample(seed)\n",
    "else:\n",
    "    # load the network weights\n",
    "    filename = \"GRUWeights.hdf5\"\n",
    "    try:\n",
    "        model.load_weights(filename)\n",
    "    except:\n",
    "        print(\"\\nUh Oh! Caught some exceptions! May be you don't have any trained and saved weights to load.\")\n",
    "        print(\"Solution: May be create and train the model anew ?\")\n",
    "        sys.exit(0)\n",
    "    Answer2 = \"y\"\n",
    "    while Answer2 == \"y\" or Answer2 == \"Y\":\n",
    "            print(\"\\nGenerating Text:\\n\")\n",
    "            # Randomly choosing a sequence from dataset to serve as a seed for sampling\n",
    "            start_index = random.randint(0, total_chars - seq_len - 1)\n",
    "            seed = dataset[start_index: start_index + seq_len]\n",
    "            sample(seed)\n",
    "            print()\n",
    "            Answer2 = raw_input(\"Generate another sample Text? (y/n): \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
